

    The skdata library provides both scripts and library routines for tasks related to data sets, including several large datasets of images that must be handled with care in order to fit in typical amounts of RAM.
    Scripts are provided for downloading, verifying, and visualizing many public benchmark datasets.
    Routines and data types are provided for interfacing to data sets as both raw and structured things.
    A low level interface provides relatively *raw* access to the contents of
    each data set, in terms that closely reflect what was (in most cases) downloaded from the web.
    Each data set may also be associated with zero or more high level interfaces.
    A high level interface provides a *sanitized* view of a data set, in which the data have been assembled into e.g. (X, y) pairs,
    in order to look like a standard machine learning problem such as classification, regression, or density estimation.
    The high level interface ensures that learning algorithms see exactly the right examples for training and testing,
    so that results are directly comparable to ones published in academic literature.



Introduction
------------

but even such widely disparate data sets can be interpreted as providing features (X) and labels (y) for a machine learning *classification problem*, and be treated


The number of conceptually different kinds of statistical inferences that dominate the attention of machine learning researchers is not so large.
Some of the standard kinds of inferences include:

* *classification* is the prediction of a discrete label from some number of real-valued or discrete-valued *feature* variables
* *regression* the prediction of real-valued responses from some number of real-valued or discrete-valued *feature* variables
* *density estimation*
* *matrix completion* often arises in user rating scenarios
* *ranking and structured prediction*

These inference categories correspond to industrially relevant applications, and algorithms for solving them have seen a lot of attention for decades.
Consequently they are supported by many mature inference algorithms (see e.g. [sklearn]_), even while research continues to develop new ones.
Other kinds of machine learning problems exist (see e.g. [LangfordReductions]_) but skdata has been developed foremost to support
the popular inference types mentioned above.

In contrast to the relatively short list of common inference types,
the number of data sets in active use for evaluating relevant inference algorithms is quite a lot larger and grows every year.
As computers get faster, storage gets cheaper, and research interests shift, we
see a steady stream of new public data sets for that focus researchers on
industrially or scientifically relevant challenges.




There is a typical pattern in the workflow of researchers as they approach a machine learning challenge.
(I am summarizing my observations of many graduate students as well as my own working habits.)

What typically happens when a machine learning researcher, say a graduate
student, starts to work on a new project is that they:

1. start with an idea for a new algorithm

1. read papers on the subject and learn what the standard benchmarks
   are

1. implement their idea and want to compare it with the previous work

1. download a relevant data set and spend a day or two ensuring that they
   have understood it correctly

1. try to determine from the paper exactly which examples were used for
   training and testing (possibly failing to do so, and guessing)

1. download another data set and spend another day or two on that one
   
1. download a third data set (etc.)

In terms of this workflow, the skdata library helps with steps 4-7.
The skdata library
Relative





Cross-validation
* training
* validation
* testing

Kfold cross-validation


Sklearn: input is sanitized


Pre-processing



Even within particular kinds of data, such as the prediction of a real valued response from another real-valued control variable,
different algorithms generalize from data differently.
For example, a linear model predicts very different values most points than a higher-order polynomial,
even if both the linear model and the polynomial have been fit to the same few examples (called "training data").

Machine learning benchmark data sets come in all shapes and sizes.


Most data sets have relevant internal structure, especially when they
represent a number of joint samples of several random variables.
It is often important to distinguish between the variables

*meta-data* of image data sets (image
sizes, compact description of semantic content), and the
*pixel* data itself. The meta-data typically is not overwhelmingly large, whereas
signal data such as image pixels, audio recordings, and video data can easily be too
large to load all at once on a single computer.





that Spice grows on the planet Dune.  Test
some maths, for example :math:`e^{\pi i} + 3 \delta`.  Or maybe an
equation on a separate line:














.. figure:: figure1.png

   This is the caption. :label:`egfig`

.. figure:: figure1.png
   :scale: 20%
   :figclass: bht

   This is the caption on a smaller figure that will be placed by default at the
   bottom of the page, and failing that it will be placed inline or at the top.
   Note that for now, scale is relative to a completely arbitrary original
   reference size which might be the original size of your image - you probably
   have to play with it. :label:`egfig2`



Cache directory
~~~~~~~~~~~~~~~

Various skdata utilities help to manage the data sets themselves, which are stored in the user's "~/.skdata" directory.





Skdata Project Architecture
---------------------------

The skdata library aims to provide two levels of interface to data sets.
The lower level interface provides a "raw" view of the underlying data set.


Skdata consists primarily of independent submodules that deal with individual data sets.
Each submodule has three important sub-sub-module files:

1. a 'dataset' file with the nitty-gritty details of how to download, extract,
   and parse a particular data set;

2. a 'view' file with any standard evaluation protocols from relevant
   literature; and

3. a 'main' file with CLI entry points for e.g. downloading and visualizing
   the data set in question.


The evaluation protocols represent the logic that turns parsed (but potentially ideosyncratic) data into one or more standardized learning tasks.


The skdata library provides two levels of interfacing to each data set
that it provides.
A low level interface provides relatively *raw* access to the contents of
a data set, in terms that reflect what was (in most cases) downloaded from the web.
The goal of the low level interface is save users the trouble of unpacking
and parsing downloaded files, while giving them direct acces to the
downloaded content.

A high level ("protocol") interface provides a sanitized version of a data
set, in which examples have been assembled into e.g. (X, y) pairs,
standard preprocessing has been applied, and the examples have been
partitioned into standard training, validation, and testing splits, where
applicable. The goal of this high level interface is to allow algorithm
designers to simply "plug in" classification and feature transformation algorithms,
and rest assured that they have trained and tested on the right examples
which allow them to make direct comparisons in academic literature.

Skdata consists primarily of independent submodules that deal with individual data sets.
Each submodule has three important sub-sub-module files:

The basic approach has been developed over years of combined experience by the authors, and used extensively in recent work (e.g. [2]).
The presentation will cover the design of data set submodules, and the basic interactions between a learning algorithm and an evaluation protocol.


Answering this question for new algorithms is often a principal contribution of research papers in machine learning.
Anyone implementing an algorithm from scientific literature must first verify that their implementation matches the results reported in that literature.


